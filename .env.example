# Decentralized Sequencer Environment Variables
# Copy this file to .env and configure as needed
#
# ⚠️ CRITICAL: PROTOCOL PARAMETERS IN PRODUCTION
# ================================================
# In production/mainnet deployments, the following parameters
# MUST be sourced from on-chain contracts and CANNOT be overridden by env vars:
#   - Submission window duration
#   - Finalization thresholds
#   - Consensus voting thresholds
#   - Validator stake requirements
#   - Batch processing parameters
#
# Environment variables for these parameters are PROVIDED FOR TESTING ONLY.
# The production sequencer will ALWAYS read these values from:
#   - Protocol State Contract
#   - Data Market Contracts
#
# This ensures protocol consistency and prevents individual validators
# from manipulating consensus parameters.
#
# SAFE TO CONFIGURE VIA ENV VARS:
#   ✅ Sequencer ID, P2P port, Redis connection
#   ✅ Component toggles (enable/disable features)
#   ✅ Worker counts and parallelization settings
#   ✅ Storage provider configuration (IPFS, etc.)
#   ✅ RPC endpoints and network settings
#   ✅ Logging and monitoring preferences

# ============================================
# CORE CONFIGURATION
# ============================================

# Unique identifier for this sequencer instance
SEQUENCER_ID=unified-sequencer-1

# P2P port for libp2p networking
P2P_PORT=9001

# Redis configuration (required for dequeuer functionality)
# Use 'redis' for Docker, 'localhost' for local binary
REDIS_HOST=redis
# External binding port - controls which host port Redis is accessible from outside Docker
# Change this if you need to access Redis from host or have port conflicts (default: 6379)
REDIS_BIND_PORT=6379
# Database number (0-15)
REDIS_DB=0
# Password - leave empty if no auth required
# WARNING: Do NOT put comments on same line as value - Docker Compose includes them!
REDIS_PASSWORD=
# Public IP of the instance where you are running this (optional for NAT traversal)
PUBLIC_IP=

# ============================================
# P2P NETWORK CONFIGURATION
# ============================================

# Connection manager settings
CONN_MANAGER_LOW_WATER=100   # Minimum number of connections to maintain
CONN_MANAGER_HIGH_WATER=400  # Maximum number of connections before pruning

# Bootstrap node multiaddr (REQUIRED for P2P)
# Format: /ip4/<IP>/tcp/<PORT>/p2p/<PEER_ID>
# Example: /ip4/YOUR.SERVER.IP.HERE/tcp/9100/p2p/YOUR_PEER_ID_HERE
BOOTSTRAP_MULTIADDR=

# Rendezvous point for peer discovery (REQUIRED for finding other peers)
# This must match what other peers are using
RENDEZVOUS_POINT=powerloom-snapshot-sequencer-network

# Private key for P2P identity (optional - will generate if not provided)
# Format: hex-encoded Ed25519 private key
# Leave empty to generate a new key on each start
PRIVATE_KEY=

# ============================================
# COMPONENT TOGGLES
# ============================================

# Enable/disable individual components (true/false)
ENABLE_LISTENER=true           # P2P gossipsub listener
ENABLE_DEQUEUER=true           # Redis queue processor
ENABLE_FINALIZER=true          # Batch finalizer
ENABLE_BATCH_AGGREGATION=true  # P2P exchange and aggregation of finalized batches
ENABLE_EVENT_MONITOR=false     # Event monitor for EpochReleased events

# ============================================
# BATCH AGGREGATION CONFIGURATION
# ============================================
# ⚠️ IMPORTANT: These values are for TESTING ONLY!
# In production/mainnet, ALL aggregation parameters MUST be read from:
#   - Protocol State Contract
#   - Data Market Contracts
# Environment variables CANNOT override on-chain parameters in production

# Batch aggregation parameters (TESTING ONLY - mainnet reads from contracts)
VOTING_THRESHOLD=0.67      # Percentage of validators required for consensus (67%)
MIN_VALIDATORS=3           # Minimum number of validators for valid consensus
BATCH_AGGREGATION_TIMEOUT=300      # Timeout for batch aggregation voting in seconds (5 minutes)
BATCH_AGGREGATION_INTERVAL=60      # How often to check batch aggregation status

# Validator settings (TESTING ONLY - mainnet reads from contracts)
VALIDATOR_STAKE_THRESHOLD=1000    # Minimum POWER tokens required to be a validator
VALIDATOR_MAX_STAKE=100000        # Maximum stake considered for voting weight

# Batch Aggregation monitoring and logging
BATCH_AGGREGATION_LOGGING_LEVEL=info      # Logging detail for batch aggregation process
BATCH_AGGREGATION_METRICS_ENABLED=true    # Enable detailed metrics for batch aggregation tracking

# Level 2 Aggregation Window (Network-wide batch collection)
# Time to wait for validator finalizations before aggregating network consensus
# First remote batch arrival starts timer, additional batches collected during window
# Window expiration triggers final Level 2 aggregation combining all validator views
AGGREGATION_WINDOW_SECONDS=30             # Wait time in seconds (default: 30s)

# Validator Priority Assignment (VPA) and On-Chain Submission
# Enable DSV nodes to submit finalized batches to ProtocolState contract using priority-based system
# When enabled, aggregator will check validator priority and submit during assigned submission windows
ENABLE_ONCHAIN_SUBMISSION=false          # Enable priority-based on-chain submission

# Note: VPA address is automatically fetched from ProtocolState contract
# No need to specify VALIDATOR_PRIORITY_ASSIGNER - it's read from validatorPriorityAssigner() method

# Note: In production deployment
# - Validator private key should be securely managed (not in env vars)
# - Contract addresses should be properly configured for your network
# - RPC nodes should be reliable and have sufficient throughput

# EIP-712 Signature Validation and Slot Authorization
# Enable validation of snapshotter addresses against protocol state cache
# When enabled, dequeuer performs the following checks:
# 1. Verify EIP-712 signature cryptographically
# 2. Ensure signature matches registered snapshotter EVM address
# 3. Validate that signer is authorized for the given slot
#
# Requirements:
# - protocol-state-cacher must be running
# - Redis populated with SlotInfo.{slotID} keys
# - Submissions must include valid EIP-712 signature
#
# Security modes:
# - false (default): Only cryptographic signature validation
# - true: Full address and slot authorization check
#
# In production: ALWAYS set to true for maximum security
ENABLE_SLOT_VALIDATION=false              # true/false (default: false)

# ============================================
# WORKER CONFIGURATION
# ============================================
# ⚠️ NOTE: Finalization thresholds and batch parameters are sourced from
# contracts in production. These env values are for testing only.

# Number of dequeuer workers (parallel submission processors)
# NOTE: Now supports multiple submission formats (single & P2PSnapshotSubmission)
DEQUEUER_WORKERS=5

# Number of finalizer workers (parallel batch processors)
# Workers now support camelCase data models and batch processing
FINALIZER_WORKERS=5

# Batch size for parallel finalization (projects per batch part)
# Supports flexible batch sizes for high and low volume epochs
FINALIZATION_BATCH_SIZE=20

# Conversion strategy for submission format (default: auto-detect)
# Options: 'auto' (recommended), 'single', 'batch'
SUBMISSION_FORMAT_STRATEGY=auto

# ============================================
# DEQUEUER CONFIGURATION
# ============================================

# For Docker Compose scaling (number of dequeuer containers)
DEQUEUER_REPLICAS=3

# ============================================
# FINALIZER CONFIGURATION
# (ONLY NEEDED IF ENABLE_FINALIZER=true)
# ============================================
# The finalizer component handles batch finalization and storage
# These settings are inherited from the centralized sequencer
# and are NOT needed for basic submission listening/processing

# Number of finalizer instances (for redundancy)
FINALIZER_REPLICAS=2

# Storage provider: ipfs, arweave, filecoin
# NOTE: Requires corresponding storage service to be running
STORAGE_PROVIDER=ipfs

# IPFS node address (ONLY if using IPFS for finalized batch storage)
# You need to run an IPFS node separately if finalizer is enabled
# Accepts multiple formats:
#   Simple: 127.0.0.1:5001 or 172.29.0.2:5001
#   URL: http://127.0.0.1:5001
#   Multiaddr: /ip4/127.0.0.1/tcp/5001
#
# For Docker with built-in IPFS: Use ./dsv.sh start --with-ipfs
# ⚠️  CRITICAL: Must run first: sudo mkdir -p /data/ipfs && sudo chown -R 1000:1000 /data/ipfs && sudo chmod -R 755 /data/ipfs
# Then set IPFS_HOST=ipfs:5001 to use the local IPFS service
#
# For Docker with external IPFS: Run ./setup-docker-network.sh first
# Then use the IP shown by: docker inspect root-ipfs-1 -f '{{index .NetworkSettings.Networks "dsv-internal-network" "IPAddress"}}'
IPFS_HOST=127.0.0.1:5001

# ============================================
# IPFS CLEANUP CONFIGURATION (for local IPFS service)
# ============================================
# These settings only apply when using the built-in IPFS service (./dsv.sh start --with-ipfs)
# Automatically unpins old CIDs to prevent storage bloat

# Maximum age for pins before cleanup (in days)
# CIDs older than this will be unpinned automatically
IPFS_CLEANUP_MAX_AGE_DAYS=7

# Cleanup interval (in hours)
# How often to run the cleanup process
IPFS_CLEANUP_INTERVAL_HOURS=72  # Every 3 days

# IPFS Data Directory Configuration
# ============================================
# Controls where IPFS data is stored on the host system
# This allows mounting IPFS data on large partitions or dedicated storage

# Host directory for IPFS data storage
# Default: /data/ipfs (use large partition when available)
# For production: Set to your high-capacity storage mount point
IPFS_DATA_DIR=/data/ipfs

# Example configurations:
# - Large partition: IPFS_DATA_DIR=/mnt/storage/ipfs
# - Dedicated disk: IPFS_DATA_DIR=/media/nvme1n1/ipfs
# - Default location: IPFS_DATA_DIR=/data/ipfs

# IPFS Port Configuration (for Docker deployment)
# ============================================
# Controls which ports are exposed from the IPFS container
# Only change if you have port conflicts on the host system

# IPFS API port (required for DSV components to communicate with IPFS)
# Default: 5001
IPFS_API_PORT=5001

# IPFS Swarm port (P2P networking - allows other IPFS nodes to connect)
# Default: 4001
IPFS_SWARM_PORT=4001

# Data availability layer: none, eigenda, celestia, avail
# For future integration with DA layers
DA_PROVIDER=none

# ============================================
# BLOCKCHAIN CONFIGURATION
# ============================================

# Powerloom Protocol Chain RPC Configuration
# All RPC interactions in this component are with the Powerloom chain only
# Option 1: Comma-separated (recommended - simplest)
POWERLOOM_RPC_NODES=http://localhost:8545,http://localhost:8546
# Option 2: JSON array with QUOTED URLs (must use quotes!)
#POWERLOOM_RPC_NODES='["http://localhost:8545","http://localhost:8546"]'

# Powerloom Archive nodes for historical queries (optional, JSON array)
POWERLOOM_ARCHIVE_RPC_NODES=[]

# Source Chain RPC Configuration (JSON array)
# where data is sourced from (e.g., Ethereum, Polygon)
SOURCE_RPC_NODES=["https://eth-mainnet.g.alchemy.com/v2/YOUR_API_KEY","https://mainnet.infura.io/v3/YOUR_KEY"]

# Source Archive nodes (optional, JSON array)
SOURCE_ARCHIVE_RPC_NODES=[]

# Protocol State Contract (REQUIRED for event monitoring)
PROTOCOL_STATE_CONTRACT=0xE88E5f64AEB483d7057645326AdDFA24A3B312DF

# Data Market Addresses (JSON array or comma-separated)
# These are the markets this sequencer will monitor
# Option 1: Comma-separated (recommended - simplest)
DATA_MARKET_ADDRESSES=0x0C2E22fe7526fAeF28E7A58c84f8723dEFcE200c
# Option 2: Multiple markets comma-separated
#DATA_MARKET_ADDRESSES=0x0C2E22fe7526fAeF28E7A58c84f8723dEFcE200c,0x21cb57C1f2352ad215a463DD867b838749CD3b8f
# Option 3: JSON array with QUOTED addresses (must use quotes around addresses!)
#DATA_MARKET_ADDRESSES='["0x0C2E22fe7526fAeF28E7A58c84f8723dEFcE200c","0x21cb57C1f2352ad215a463DD867b838749CD3b8f"]'

# ============================================
# EVENT MONITORING CONFIGURATION
# ============================================

# Path to the Legacy Protocol State Contract ABI JSON file
# This file contains the ABI for parsing EpochReleased events from legacy contracts
CONTRACT_ABI_PATH=./abi/LegacyProtocolState.json

# ABI File Naming Convention:
# - LegacyProtocolState.json: OLD/LEGACY ProtocolState contract (pre-VPA)
# - PowerloomProtocolState.json: NEW ProtocolState contract (VPA-enabled)
# - ValidatorPriorityAssigner.json: VPA contract for priority assignment events

# Submission window duration in seconds (how long to accept submissions after epoch)
# ⚠️ TESTING ONLY: In production, this MUST be read from Protocol State Contract
# This env var is ONLY for local testing/development
# Production deployments will IGNORE this value and use on-chain parameters
SUBMISSION_WINDOW_DURATION=60

# Maximum concurrent submission windows
MAX_CONCURRENT_WINDOWS=100

# Event polling interval in seconds
EVENT_POLL_INTERVAL=12

# Starting block for event monitoring 
# 0 = start from current block (recommended for production)
# Any positive number = start from that specific block
EVENT_START_BLOCK=0

# Number of blocks to process in batch
EVENT_BLOCK_BATCH_SIZE=1000

# ============================================
# IDENTITY & VERIFICATION
# ============================================

# Full node addresses (JSON array or comma-separated)
# These addresses bypass certain verification checks by the dequeuer
FULL_NODE_ADDRESSES=

# Skip identity verification (for testing only)
SKIP_IDENTITY_VERIFICATION=false

# Check for flagged snapshotters
CHECK_FLAGGED_SNAPSHOTTERS=true

# Verification cache TTL in seconds
VERIFICATION_CACHE_TTL=600


# ============================================
# DEDUPLICATION CONFIGURATION
# ============================================

# Enable deduplication
DEDUP_ENABLED=true

# Local cache size for deduplication
DEDUP_LOCAL_CACHE_SIZE=10000

# Deduplication TTL in seconds
DEDUP_TTL_SECONDS=7200

# ============================================
# PERFORMANCE TUNING
# ============================================

# Connection manager settings
CONN_MANAGER_LOW_WATER=100
CONN_MANAGER_HIGH_WATER=400

# Gossipsub heartbeat interval in milliseconds
GOSSIPSUB_HEARTBEAT_MS=700

# Maximum submissions per epoch
MAX_SUBMISSIONS_PER_EPOCH=100

# ============================================
# DEBUGGING & MONITORING
# ============================================

# Enable debug logging (true/false)
DEBUG_MODE=false
LOG_LEVEL=info

# Metrics port for Prometheus
METRICS_PORT=9090
METRICS_ENABLED=false

# Slack webhook for alerts (optional)
SLACK_WEBHOOK_URL=

# Enable advanced pipeline health monitoring
# Provides more detailed alerts and tracking for batch processing stages
PIPELINE_HEALTH_MONITORING=false

# API configuration
API_HOST=0.0.0.0
API_PORT=8080
API_AUTH_TOKEN=

# Monitor API configuration
MONITOR_API_PORT=8080

# Grafana configuration (for monitoring profile)
# Port for Grafana web interface (default: 3000)
GRAFANA_PORT=3000

# Grafana admin password
GRAFANA_PASSWORD=admin

# ============================================
# GOSSIPSUB TOPIC CONFIGURATION
# ============================================
# Configure gossipsub topic names for P2P networking
# These allow customization of topic names for different deployment scenarios
# Snapshot submission topics (discovery and main submissions)
# Format: {prefix}/0 (discovery), {prefix}/all (submissions)
GOSSIPSUB_SNAPSHOT_SUBMISSION_PREFIX=/powerloom/snapshot-submissions

# Finalized batch topics (discovery and batch exchange)
# Format: {prefix}/0 (discovery), {prefix}/all (batches)
GOSSIPSUB_FINALIZED_BATCH_PREFIX=/powerloom/finalized-batches

# Validator consensus topics
# Presence topic for validator heartbeat and discovery
GOSSIPSUB_VALIDATOR_PRESENCE_TOPIC=/powerloom/validator/presence

# Consensus voting and proposal topics
# Used for validator consensus and batch aggregation coordination
GOSSIPSUB_CONSENSUS_VOTES_TOPIC=/powerloom/consensus/votes
GOSSIPSUB_CONSENSUS_PROPOSALS_TOPIC=/powerloom/consensus/proposals


# ============================================
# QUICK START CONFIGURATIONS
# ============================================

# For LOCAL TESTING (everything on one machine):
# - Use defaults above
# - Set REDIS_ADDR=localhost:6379
# - Leave PRIVATE_KEY empty (will auto-generate)
# - Set DEBUG_MODE=true

# For DOCKER COMPOSE:
# - Set REDIS_ADDR=redis:6379
# - Set IPFS_HOST=ipfs:5001
# - Use the bootstrap multiaddr above

# For PRODUCTION:
# - Generate unique PRIVATE_KEY for each instance
# - Set proper RPC_URL with your API key

# ============================================
# NEW CONTRACT CONFIGURATION (VPA-ENABLED DEPLOYMENT)
# ============================================

# Enable submission to new VPA-enabled contracts
# When true, DSV submits to new contracts via relayer-py
# When false (default), DSV does NO contract submission
# Legacy contracts are handled by separate snapshotter-lite-v2 workflow directly to centralized sequencer
USE_NEW_CONTRACTS=false

# New Protocol State contract address (VPA-enabled)
# This contract has ValidatorPriorityAssigner linked internally
NEW_PROTOCOL_STATE_CONTRACT=

# New Data Market contract address (VPA-enabled)
# This contract works with the new Protocol State contract
NEW_DATA_MARKET_CONTRACT=

# relayer-py service endpoint for submitting batches to new contracts
# When USE_NEW_CONTRACTS=true, aggregator calls relayer-py HTTP API
RELAYER_PY_ENDPOINT=http://relayer-py:8080

# VPA validator address for priority checking
# This validator's address to check if it has priority for current epoch
# Required when USE_NEW_CONTRACTS=true for priority-based submissions
VPA_VALIDATOR_ADDRESS=

# VPA contract address for priority monitoring (ValidatorPriorityAssigner)
# TODO: This should be fetched from ProtocolState. For now, specify manually.
VPA_CONTRACT_ADDRESS=

# ============================================
# RELAYER-PY CONFIGURATION NOTES
# ============================================

# relayer-py uses its own settings.json for configuration
# The only thing we need to override is the protocol state contract address
# All other settings (RPC, signers, etc.) are already configured in relayer-py/settings/

# To customize signers or RPC settings, edit the appropriate settings file in relayer-py/settings/

# VPA priority event monitoring is automatically enabled when:
# - USE_NEW_CONTRACTS=true
# - VPA_VALIDATOR_ADDRESS is set
# - NEW_PROTOCOL_STATE_CONTRACT is set (VPA address fetched from this contract)

# ============================================
# NEW CONTRACT SUBMISSION QUICK START EXAMPLES
# ============================================

# For TESTING New Contracts (DSV submission enabled):
# - Set USE_NEW_CONTRACTS=true
# - Configure NEW_PROTOCOL_STATE_CONTRACT and NEW_DATA_MARKET_CONTRACT addresses
# - Start with ./dsv.sh start --with-vpa --with-ipfs
# (relayer-py will use its default settings.json)
# IMPORTANT: DSV does NO contract submission when USE_NEW_CONTRACTS=false

# For TESTING (Environment-based Signers):
# - Set USE_NEW_CONTRACTS=true
# - Configure NEW_*_CONTRACT addresses
# - Set RELAYER_SIGNER_ADDRESSES and RELAYER_SIGNER_PRIVATE_KEYS (comma-separated)
# - Start with ./dsv.sh start --with-vpa --with-ipfs
# (signers configured via environment variables)

# For PRODUCTION:
# - Configure relayer-py/settings.json with production signers and RPC endpoints
# - Or use RELAYER_SIGNER_* environment variables for automated deployment
# - Monitor submission success rates via aggregator and relayer-py logs
# - Set DEBUG_MODE=false
# - Configure STORAGE_PROVIDER and DA_PROVIDER as needed